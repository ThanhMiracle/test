{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "46da14a4-fcfb-44e7-a909-028f576a22b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.functional import softmax\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9f010988-b261-407c-a820-4d0b84ce8d23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"IMDB_Dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "326c5fa8-787a-4dcb-a943-a3310b084399",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haill\\AppData\\Local\\Temp\\ipykernel_10372\\3798466707.py:3: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the review column\n",
    "data['cleaned_review'] = data['review'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5b272d77-a974-4022-b47b-7c5294861580",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Convert sentiment labels to numerical format\n",
    "data['sentiment'] = data['sentiment'].map({'positive': 1, 'negative': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "536a6d5d-de10-46b9-8d6f-b94dd3bd5b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.cleaned_review\n",
    "        self.targets = self.data.sentiment\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n",
    "            'mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a45bad56-a51c-4cf7-a79f-85325c293c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 512  # You can adjust this depending on your specific needs\n",
    "\n",
    "# Create an instance of the dataset\n",
    "dataset = SentimentDataset(data, tokenizer, max_len)\n",
    "\n",
    "# Example: Creating a DataLoader for training\n",
    "train_dataloader = DataLoader(dataset, batch_size=16, shuffle=True)  # Adjust batch size as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "518daa12-1ee9-40f5-9e56-bafd69bc7f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, 2)  # Assuming binary classification\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1e77fad8-8746-43f0-882d-6732452035c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\code\\NLP\\Transformers\\New folder\\env\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "ntokens = len(tokenizer.vocab)  # size of vocabulary\n",
    "emsize = 200  # embedding dimension\n",
    "nhid = 200  # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2  # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2  # the number of heads in the multiheadattention models\n",
    "dropout = 0.2  # the dropout value\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4be1f3ad-d656-4614-91cc-6604a8d5b787",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_loader = iter(train_dataloader)\n",
    "first_batch = next(iter_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "cbe4275d-2dc4-4c88-9b06-19df9b519f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = first_batch['ids']\n",
    "mask = first_batch['mask']\n",
    "target = first_batch['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "af47e0ff-a5cb-4336-a2e7-ab386338d091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 512])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a4c26878-5825-4858-ba12-d9f707ad4b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_mask = model.generate_square_subsequent_mask(input.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "01187b60-a409-40ad-bf43-b4daa5c533f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 16])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "03e5809a-afa9-4311-982d-789951b7baa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(input, src_mask)\n",
    "out = out[:,0,:]\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ae04f269-6f11-4766-9a4b-649054cda87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3bac7b19-5a70-402a-bba3-cb3cc71e2047",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(out , target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "13f1456e-6a00-4b9e-8a32-b2772faba1be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |     0/ 3125 iteration | loss 0.6807945966720581 |\n",
      "| epoch   1 |     1/ 3125 iteration | loss 0.8245986700057983 |\n",
      "| epoch   1 |     2/ 3125 iteration | loss 0.9267174005508423 |\n",
      "| epoch   1 |     3/ 3125 iteration | loss 0.7987486124038696 |\n",
      "| epoch   1 |     4/ 3125 iteration | loss 0.8654099106788635 |\n",
      "| epoch   1 |     5/ 3125 iteration | loss 0.9166464805603027 |\n",
      "| epoch   1 |     6/ 3125 iteration | loss 0.6894135475158691 |\n",
      "| epoch   1 |     7/ 3125 iteration | loss 0.7799966931343079 |\n",
      "| epoch   1 |     8/ 3125 iteration | loss 0.7373149394989014 |\n",
      "| epoch   1 |     9/ 3125 iteration | loss 0.7779186367988586 |\n",
      "| epoch   1 |    10/ 3125 iteration | loss 0.7196653485298157 |\n",
      "| epoch   1 |    11/ 3125 iteration | loss 0.6378631591796875 |\n",
      "| epoch   1 |    12/ 3125 iteration | loss 0.7123527526855469 |\n",
      "| epoch   1 |    13/ 3125 iteration | loss 0.663116991519928 |\n",
      "| epoch   1 |    14/ 3125 iteration | loss 0.8183684945106506 |\n",
      "| epoch   1 |    15/ 3125 iteration | loss 0.7570123076438904 |\n",
      "| epoch   1 |    16/ 3125 iteration | loss 0.6498714089393616 |\n",
      "| epoch   1 |    17/ 3125 iteration | loss 0.7508556842803955 |\n",
      "| epoch   1 |    18/ 3125 iteration | loss 0.6980693340301514 |\n",
      "| epoch   1 |    19/ 3125 iteration | loss 0.6796213388442993 |\n",
      "| epoch   1 |    20/ 3125 iteration | loss 0.7749847769737244 |\n",
      "| epoch   1 |    21/ 3125 iteration | loss 0.7236124873161316 |\n",
      "| epoch   1 |    22/ 3125 iteration | loss 0.7086195945739746 |\n",
      "| epoch   1 |    23/ 3125 iteration | loss 0.7067686915397644 |\n",
      "| epoch   1 |    24/ 3125 iteration | loss 0.6589140892028809 |\n",
      "| epoch   1 |    25/ 3125 iteration | loss 0.6872154474258423 |\n",
      "| epoch   1 |    26/ 3125 iteration | loss 0.7396940588951111 |\n",
      "| epoch   1 |    27/ 3125 iteration | loss 0.7371736168861389 |\n",
      "| epoch   1 |    28/ 3125 iteration | loss 0.7163423895835876 |\n",
      "| epoch   1 |    29/ 3125 iteration | loss 0.7044521570205688 |\n",
      "| epoch   1 |    30/ 3125 iteration | loss 0.692988395690918 |\n",
      "| epoch   1 |    31/ 3125 iteration | loss 0.7293822765350342 |\n",
      "| epoch   1 |    32/ 3125 iteration | loss 0.7696003317832947 |\n",
      "| epoch   1 |    33/ 3125 iteration | loss 0.743053138256073 |\n",
      "| epoch   1 |    34/ 3125 iteration | loss 0.6385337114334106 |\n",
      "| epoch   1 |    35/ 3125 iteration | loss 0.7242299318313599 |\n",
      "| epoch   1 |    36/ 3125 iteration | loss 0.6812599897384644 |\n",
      "| epoch   1 |    37/ 3125 iteration | loss 0.7464287877082825 |\n",
      "| epoch   1 |    38/ 3125 iteration | loss 0.6760080456733704 |\n",
      "| epoch   1 |    39/ 3125 iteration | loss 0.6996088027954102 |\n",
      "| epoch   1 |    40/ 3125 iteration | loss 0.6846214532852173 |\n",
      "| epoch   1 |    41/ 3125 iteration | loss 0.6677276492118835 |\n",
      "| epoch   1 |    42/ 3125 iteration | loss 0.7207944393157959 |\n",
      "| epoch   1 |    43/ 3125 iteration | loss 0.6820173263549805 |\n",
      "| epoch   1 |    44/ 3125 iteration | loss 0.6857025623321533 |\n",
      "| epoch   1 |    45/ 3125 iteration | loss 0.733400821685791 |\n",
      "| epoch   1 |    46/ 3125 iteration | loss 0.7020446062088013 |\n",
      "| epoch   1 |    47/ 3125 iteration | loss 0.7116276025772095 |\n",
      "| epoch   1 |    48/ 3125 iteration | loss 0.6633527278900146 |\n",
      "| epoch   1 |    49/ 3125 iteration | loss 0.7110574245452881 |\n",
      "| epoch   1 |    50/ 3125 iteration | loss 0.6762800216674805 |\n",
      "| epoch   1 |    51/ 3125 iteration | loss 0.7187091112136841 |\n",
      "| epoch   1 |    52/ 3125 iteration | loss 0.6347262263298035 |\n",
      "| epoch   1 |    53/ 3125 iteration | loss 0.7218925952911377 |\n",
      "| epoch   1 |    54/ 3125 iteration | loss 0.7051606178283691 |\n",
      "| epoch   1 |    55/ 3125 iteration | loss 0.6668998003005981 |\n",
      "| epoch   1 |    56/ 3125 iteration | loss 0.7849642634391785 |\n",
      "| epoch   1 |    57/ 3125 iteration | loss 0.8365648984909058 |\n",
      "| epoch   1 |    58/ 3125 iteration | loss 0.609581470489502 |\n",
      "| epoch   1 |    59/ 3125 iteration | loss 0.6882365942001343 |\n",
      "| epoch   1 |    60/ 3125 iteration | loss 0.7162944674491882 |\n",
      "| epoch   1 |    61/ 3125 iteration | loss 0.7428149580955505 |\n",
      "| epoch   1 |    62/ 3125 iteration | loss 0.6557210087776184 |\n",
      "| epoch   1 |    63/ 3125 iteration | loss 0.7606318593025208 |\n",
      "| epoch   1 |    64/ 3125 iteration | loss 0.7223526239395142 |\n",
      "| epoch   1 |    65/ 3125 iteration | loss 0.6467065811157227 |\n",
      "| epoch   1 |    66/ 3125 iteration | loss 0.6429075002670288 |\n",
      "| epoch   1 |    67/ 3125 iteration | loss 0.7424379587173462 |\n",
      "| epoch   1 |    68/ 3125 iteration | loss 0.9259324669837952 |\n",
      "| epoch   1 |    69/ 3125 iteration | loss 0.723886251449585 |\n",
      "| epoch   1 |    70/ 3125 iteration | loss 0.6929139494895935 |\n",
      "| epoch   1 |    71/ 3125 iteration | loss 0.8549630045890808 |\n",
      "| epoch   1 |    72/ 3125 iteration | loss 0.6935092210769653 |\n",
      "| epoch   1 |    73/ 3125 iteration | loss 0.781711995601654 |\n",
      "| epoch   1 |    74/ 3125 iteration | loss 0.6662859916687012 |\n",
      "| epoch   1 |    75/ 3125 iteration | loss 0.8203402161598206 |\n",
      "| epoch   1 |    76/ 3125 iteration | loss 0.7200971841812134 |\n",
      "| epoch   1 |    77/ 3125 iteration | loss 0.6794731616973877 |\n",
      "| epoch   1 |    78/ 3125 iteration | loss 0.7550477981567383 |\n",
      "| epoch   1 |    79/ 3125 iteration | loss 0.7418027520179749 |\n",
      "| epoch   1 |    80/ 3125 iteration | loss 0.8100361227989197 |\n",
      "| epoch   1 |    81/ 3125 iteration | loss 0.7857435345649719 |\n",
      "| epoch   1 |    82/ 3125 iteration | loss 0.7508187294006348 |\n",
      "| epoch   1 |    83/ 3125 iteration | loss 0.7403276562690735 |\n",
      "| epoch   1 |    84/ 3125 iteration | loss 0.7262371182441711 |\n",
      "| epoch   1 |    85/ 3125 iteration | loss 0.7685271501541138 |\n",
      "| epoch   1 |    86/ 3125 iteration | loss 0.8376001119613647 |\n",
      "| epoch   1 |    87/ 3125 iteration | loss 0.6623320579528809 |\n",
      "| epoch   1 |    88/ 3125 iteration | loss 0.7071570754051208 |\n",
      "| epoch   1 |    89/ 3125 iteration | loss 0.7018445134162903 |\n",
      "| epoch   1 |    90/ 3125 iteration | loss 0.6532041430473328 |\n",
      "| epoch   1 |    91/ 3125 iteration | loss 0.6981210112571716 |\n",
      "| epoch   1 |    92/ 3125 iteration | loss 0.6754967570304871 |\n",
      "| epoch   1 |    93/ 3125 iteration | loss 0.6869537234306335 |\n",
      "| epoch   1 |    94/ 3125 iteration | loss 0.7022236585617065 |\n",
      "| epoch   1 |    95/ 3125 iteration | loss 0.7252490520477295 |\n",
      "| epoch   1 |    96/ 3125 iteration | loss 0.7462130784988403 |\n",
      "| epoch   1 |    97/ 3125 iteration | loss 0.6792092323303223 |\n",
      "| epoch   1 |    98/ 3125 iteration | loss 0.7371845841407776 |\n",
      "| epoch   1 |    99/ 3125 iteration | loss 0.6921523809432983 |\n",
      "| epoch   1 |   100/ 3125 iteration | loss 0.6990481615066528 |\n",
      "| epoch   1 |   101/ 3125 iteration | loss 0.7063072323799133 |\n",
      "| epoch   1 |   102/ 3125 iteration | loss 0.7516602277755737 |\n",
      "| epoch   1 |   103/ 3125 iteration | loss 0.7367944717407227 |\n",
      "| epoch   1 |   104/ 3125 iteration | loss 0.6736050248146057 |\n",
      "| epoch   1 |   105/ 3125 iteration | loss 0.7323614954948425 |\n",
      "| epoch   1 |   106/ 3125 iteration | loss 0.6875193119049072 |\n",
      "| epoch   1 |   107/ 3125 iteration | loss 0.6995319724082947 |\n",
      "| epoch   1 |   108/ 3125 iteration | loss 0.7261497974395752 |\n",
      "| epoch   1 |   109/ 3125 iteration | loss 0.6992053389549255 |\n",
      "| epoch   1 |   110/ 3125 iteration | loss 0.6824500560760498 |\n",
      "| epoch   1 |   111/ 3125 iteration | loss 0.7319959998130798 |\n",
      "| epoch   1 |   112/ 3125 iteration | loss 0.6833145618438721 |\n",
      "| epoch   1 |   113/ 3125 iteration | loss 0.7350508570671082 |\n",
      "| epoch   1 |   114/ 3125 iteration | loss 0.6360071897506714 |\n",
      "| epoch   1 |   115/ 3125 iteration | loss 0.753135085105896 |\n",
      "| epoch   1 |   116/ 3125 iteration | loss 0.686834990978241 |\n",
      "| epoch   1 |   117/ 3125 iteration | loss 0.685158908367157 |\n",
      "| epoch   1 |   118/ 3125 iteration | loss 0.6404167413711548 |\n",
      "| epoch   1 |   119/ 3125 iteration | loss 0.7057437896728516 |\n",
      "| epoch   1 |   120/ 3125 iteration | loss 0.6728672385215759 |\n",
      "| epoch   1 |   121/ 3125 iteration | loss 0.6943846940994263 |\n",
      "| epoch   1 |   122/ 3125 iteration | loss 0.6929150819778442 |\n",
      "| epoch   1 |   123/ 3125 iteration | loss 0.7627168297767639 |\n",
      "| epoch   1 |   124/ 3125 iteration | loss 0.7082853317260742 |\n",
      "| epoch   1 |   125/ 3125 iteration | loss 0.6887631416320801 |\n",
      "| epoch   1 |   126/ 3125 iteration | loss 0.7661995887756348 |\n",
      "| epoch   1 |   127/ 3125 iteration | loss 0.6556711792945862 |\n",
      "| epoch   1 |   128/ 3125 iteration | loss 0.7945494055747986 |\n",
      "| epoch   1 |   129/ 3125 iteration | loss 0.741071343421936 |\n",
      "| epoch   1 |   130/ 3125 iteration | loss 0.7595504522323608 |\n",
      "| epoch   1 |   131/ 3125 iteration | loss 0.6897016763687134 |\n",
      "| epoch   1 |   132/ 3125 iteration | loss 0.7616701722145081 |\n",
      "| epoch   1 |   133/ 3125 iteration | loss 0.7807847261428833 |\n",
      "| epoch   1 |   134/ 3125 iteration | loss 0.6952399015426636 |\n",
      "| epoch   1 |   135/ 3125 iteration | loss 0.6730579137802124 |\n",
      "| epoch   1 |   136/ 3125 iteration | loss 0.6743754744529724 |\n",
      "| epoch   1 |   137/ 3125 iteration | loss 0.716342031955719 |\n",
      "| epoch   1 |   138/ 3125 iteration | loss 0.7267484068870544 |\n",
      "| epoch   1 |   139/ 3125 iteration | loss 0.7028884887695312 |\n",
      "| epoch   1 |   140/ 3125 iteration | loss 0.7736912369728088 |\n",
      "| epoch   1 |   141/ 3125 iteration | loss 0.7651932835578918 |\n",
      "| epoch   1 |   142/ 3125 iteration | loss 0.6851373314857483 |\n",
      "| epoch   1 |   143/ 3125 iteration | loss 0.747908353805542 |\n",
      "| epoch   1 |   144/ 3125 iteration | loss 0.6123669743537903 |\n",
      "| epoch   1 |   145/ 3125 iteration | loss 0.9836646914482117 |\n",
      "| epoch   1 |   146/ 3125 iteration | loss 0.7054439783096313 |\n",
      "| epoch   1 |   147/ 3125 iteration | loss 0.6822219491004944 |\n",
      "| epoch   1 |   148/ 3125 iteration | loss 0.802480936050415 |\n",
      "| epoch   1 |   149/ 3125 iteration | loss 0.7801146507263184 |\n",
      "| epoch   1 |   150/ 3125 iteration | loss 0.6766663193702698 |\n",
      "| epoch   1 |   151/ 3125 iteration | loss 0.6827389001846313 |\n",
      "| epoch   1 |   152/ 3125 iteration | loss 0.6111070513725281 |\n",
      "| epoch   1 |   153/ 3125 iteration | loss 0.8053675293922424 |\n",
      "| epoch   1 |   154/ 3125 iteration | loss 0.6617063283920288 |\n",
      "| epoch   1 |   155/ 3125 iteration | loss 0.7848398089408875 |\n",
      "| epoch   1 |   156/ 3125 iteration | loss 0.6961801648139954 |\n",
      "| epoch   1 |   157/ 3125 iteration | loss 0.7442412972450256 |\n",
      "| epoch   1 |   158/ 3125 iteration | loss 0.6438665390014648 |\n",
      "| epoch   1 |   159/ 3125 iteration | loss 0.8688910007476807 |\n",
      "| epoch   1 |   160/ 3125 iteration | loss 0.7967875599861145 |\n",
      "| epoch   1 |   161/ 3125 iteration | loss 0.691819965839386 |\n",
      "| epoch   1 |   162/ 3125 iteration | loss 0.6498821377754211 |\n",
      "| epoch   1 |   163/ 3125 iteration | loss 0.7976890802383423 |\n",
      "| epoch   1 |   164/ 3125 iteration | loss 0.8270937204360962 |\n",
      "| epoch   1 |   165/ 3125 iteration | loss 0.4918299913406372 |\n",
      "| epoch   1 |   166/ 3125 iteration | loss 0.6129161715507507 |\n",
      "| epoch   1 |   167/ 3125 iteration | loss 0.9844211339950562 |\n",
      "| epoch   1 |   168/ 3125 iteration | loss 0.8636003136634827 |\n",
      "| epoch   1 |   169/ 3125 iteration | loss 0.6676668524742126 |\n",
      "| epoch   1 |   170/ 3125 iteration | loss 0.6162059903144836 |\n",
      "| epoch   1 |   171/ 3125 iteration | loss 0.8491204977035522 |\n",
      "| epoch   1 |   172/ 3125 iteration | loss 1.0223942995071411 |\n",
      "| epoch   1 |   173/ 3125 iteration | loss 0.8226044178009033 |\n",
      "| epoch   1 |   174/ 3125 iteration | loss 0.6815680265426636 |\n",
      "| epoch   1 |   175/ 3125 iteration | loss 0.6530047059059143 |\n",
      "| epoch   1 |   176/ 3125 iteration | loss 0.6031049489974976 |\n",
      "| epoch   1 |   177/ 3125 iteration | loss 0.7386474609375 |\n",
      "| epoch   1 |   178/ 3125 iteration | loss 0.6745471358299255 |\n",
      "| epoch   1 |   179/ 3125 iteration | loss 0.7265107035636902 |\n",
      "| epoch   1 |   180/ 3125 iteration | loss 0.7030012011528015 |\n",
      "| epoch   1 |   181/ 3125 iteration | loss 0.7326420545578003 |\n",
      "| epoch   1 |   182/ 3125 iteration | loss 0.725268542766571 |\n",
      "| epoch   1 |   183/ 3125 iteration | loss 0.6432272791862488 |\n",
      "| epoch   1 |   184/ 3125 iteration | loss 0.7410470843315125 |\n",
      "| epoch   1 |   185/ 3125 iteration | loss 0.7020734548568726 |\n",
      "| epoch   1 |   186/ 3125 iteration | loss 0.691620945930481 |\n",
      "| epoch   1 |   187/ 3125 iteration | loss 0.680554211139679 |\n",
      "| epoch   1 |   188/ 3125 iteration | loss 0.6899327039718628 |\n",
      "| epoch   1 |   189/ 3125 iteration | loss 0.6959187388420105 |\n",
      "| epoch   1 |   190/ 3125 iteration | loss 0.6950314044952393 |\n",
      "| epoch   1 |   191/ 3125 iteration | loss 0.7791287899017334 |\n",
      "| epoch   1 |   192/ 3125 iteration | loss 0.7022800445556641 |\n",
      "| epoch   1 |   193/ 3125 iteration | loss 0.711696445941925 |\n",
      "| epoch   1 |   194/ 3125 iteration | loss 0.663705050945282 |\n",
      "| epoch   1 |   195/ 3125 iteration | loss 0.7044190168380737 |\n",
      "| epoch   1 |   196/ 3125 iteration | loss 0.7140903472900391 |\n",
      "| epoch   1 |   197/ 3125 iteration | loss 0.6847015023231506 |\n",
      "| epoch   1 |   198/ 3125 iteration | loss 0.7192273139953613 |\n",
      "| epoch   1 |   199/ 3125 iteration | loss 0.7222651243209839 |\n",
      "| epoch   1 |   200/ 3125 iteration | loss 0.6543060541152954 |\n",
      "| epoch   1 |   201/ 3125 iteration | loss 0.6849476099014282 |\n",
      "| epoch   1 |   202/ 3125 iteration | loss 0.7073313593864441 |\n",
      "| epoch   1 |   203/ 3125 iteration | loss 0.7256178855895996 |\n",
      "| epoch   1 |   204/ 3125 iteration | loss 0.6693615317344666 |\n",
      "| epoch   1 |   205/ 3125 iteration | loss 0.69188392162323 |\n",
      "| epoch   1 |   206/ 3125 iteration | loss 0.6734867691993713 |\n",
      "| epoch   1 |   207/ 3125 iteration | loss 0.6844571232795715 |\n",
      "| epoch   1 |   208/ 3125 iteration | loss 0.7490635514259338 |\n",
      "| epoch   1 |   209/ 3125 iteration | loss 0.7251894474029541 |\n",
      "| epoch   1 |   210/ 3125 iteration | loss 0.6614142060279846 |\n",
      "| epoch   1 |   211/ 3125 iteration | loss 0.7371426224708557 |\n",
      "| epoch   1 |   212/ 3125 iteration | loss 0.7005173563957214 |\n",
      "| epoch   1 |   213/ 3125 iteration | loss 0.6724109649658203 |\n",
      "| epoch   1 |   214/ 3125 iteration | loss 0.6735156774520874 |\n",
      "| epoch   1 |   215/ 3125 iteration | loss 0.8126392960548401 |\n",
      "| epoch   1 |   216/ 3125 iteration | loss 0.7393878102302551 |\n",
      "| epoch   1 |   217/ 3125 iteration | loss 0.7453246116638184 |\n",
      "| epoch   1 |   218/ 3125 iteration | loss 0.7454245686531067 |\n",
      "| epoch   1 |   219/ 3125 iteration | loss 0.8126109838485718 |\n",
      "| epoch   1 |   220/ 3125 iteration | loss 0.7818564772605896 |\n",
      "| epoch   1 |   221/ 3125 iteration | loss 0.7378710508346558 |\n",
      "| epoch   1 |   222/ 3125 iteration | loss 0.6754799485206604 |\n",
      "| epoch   1 |   223/ 3125 iteration | loss 0.6974280476570129 |\n",
      "| epoch   1 |   224/ 3125 iteration | loss 0.7633090615272522 |\n",
      "| epoch   1 |   225/ 3125 iteration | loss 0.7337666749954224 |\n",
      "| epoch   1 |   226/ 3125 iteration | loss 0.6989319324493408 |\n",
      "| epoch   1 |   227/ 3125 iteration | loss 0.7592145800590515 |\n",
      "| epoch   1 |   228/ 3125 iteration | loss 0.7224733829498291 |\n",
      "| epoch   1 |   229/ 3125 iteration | loss 0.656619131565094 |\n",
      "| epoch   1 |   230/ 3125 iteration | loss 0.6854079365730286 |\n",
      "| epoch   1 |   231/ 3125 iteration | loss 0.7309986352920532 |\n",
      "| epoch   1 |   232/ 3125 iteration | loss 0.7201400399208069 |\n",
      "| epoch   1 |   233/ 3125 iteration | loss 0.6798732280731201 |\n",
      "| epoch   1 |   234/ 3125 iteration | loss 0.6748575568199158 |\n",
      "| epoch   1 |   235/ 3125 iteration | loss 0.7011919021606445 |\n",
      "| epoch   1 |   236/ 3125 iteration | loss 0.6996707916259766 |\n",
      "| epoch   1 |   237/ 3125 iteration | loss 0.7047666311264038 |\n",
      "| epoch   1 |   238/ 3125 iteration | loss 0.6888518333435059 |\n",
      "| epoch   1 |   239/ 3125 iteration | loss 0.6649472713470459 |\n",
      "| epoch   1 |   240/ 3125 iteration | loss 0.6496338844299316 |\n",
      "| epoch   1 |   241/ 3125 iteration | loss 0.7761176824569702 |\n",
      "| epoch   1 |   242/ 3125 iteration | loss 0.7260724902153015 |\n",
      "| epoch   1 |   243/ 3125 iteration | loss 0.6908952593803406 |\n",
      "| epoch   1 |   244/ 3125 iteration | loss 0.7499518394470215 |\n",
      "| epoch   1 |   245/ 3125 iteration | loss 0.6775946617126465 |\n",
      "| epoch   1 |   246/ 3125 iteration | loss 0.7207896113395691 |\n",
      "| epoch   1 |   247/ 3125 iteration | loss 0.6930902004241943 |\n",
      "| epoch   1 |   248/ 3125 iteration | loss 0.705590546131134 |\n",
      "| epoch   1 |   249/ 3125 iteration | loss 0.6812708973884583 |\n",
      "| epoch   1 |   250/ 3125 iteration | loss 0.676561176776886 |\n",
      "| epoch   1 |   251/ 3125 iteration | loss 0.7174533009529114 |\n",
      "| epoch   1 |   252/ 3125 iteration | loss 0.6592783331871033 |\n",
      "| epoch   1 |   253/ 3125 iteration | loss 0.7381409406661987 |\n",
      "| epoch   1 |   254/ 3125 iteration | loss 0.7060673236846924 |\n",
      "| epoch   1 |   255/ 3125 iteration | loss 0.7128528356552124 |\n",
      "| epoch   1 |   256/ 3125 iteration | loss 0.6921828985214233 |\n",
      "| epoch   1 |   257/ 3125 iteration | loss 0.6691369414329529 |\n",
      "| epoch   1 |   258/ 3125 iteration | loss 0.7032758593559265 |\n",
      "| epoch   1 |   259/ 3125 iteration | loss 0.714648425579071 |\n",
      "| epoch   1 |   260/ 3125 iteration | loss 0.6714220643043518 |\n",
      "| epoch   1 |   261/ 3125 iteration | loss 0.6465866565704346 |\n",
      "| epoch   1 |   262/ 3125 iteration | loss 0.7279262542724609 |\n",
      "| epoch   1 |   263/ 3125 iteration | loss 0.7019835710525513 |\n",
      "| epoch   1 |   264/ 3125 iteration | loss 0.771579921245575 |\n",
      "| epoch   1 |   265/ 3125 iteration | loss 0.7505414485931396 |\n",
      "| epoch   1 |   266/ 3125 iteration | loss 0.664530873298645 |\n",
      "| epoch   1 |   267/ 3125 iteration | loss 0.7317289113998413 |\n",
      "| epoch   1 |   268/ 3125 iteration | loss 0.6873087286949158 |\n",
      "| epoch   1 |   269/ 3125 iteration | loss 0.7159596681594849 |\n",
      "| epoch   1 |   270/ 3125 iteration | loss 0.7078031301498413 |\n",
      "| epoch   1 |   271/ 3125 iteration | loss 0.6910448670387268 |\n",
      "| epoch   1 |   272/ 3125 iteration | loss 0.6862925291061401 |\n",
      "| epoch   1 |   273/ 3125 iteration | loss 0.6863380074501038 |\n",
      "| epoch   1 |   274/ 3125 iteration | loss 0.6778219938278198 |\n",
      "| epoch   1 |   275/ 3125 iteration | loss 0.7214508652687073 |\n",
      "| epoch   1 |   276/ 3125 iteration | loss 0.7474665641784668 |\n",
      "| epoch   1 |   277/ 3125 iteration | loss 0.7521845698356628 |\n",
      "| epoch   1 |   278/ 3125 iteration | loss 0.7043904662132263 |\n",
      "| epoch   1 |   279/ 3125 iteration | loss 0.7295932769775391 |\n",
      "| epoch   1 |   280/ 3125 iteration | loss 0.6869019865989685 |\n",
      "| epoch   1 |   281/ 3125 iteration | loss 0.7203841805458069 |\n",
      "| epoch   1 |   282/ 3125 iteration | loss 0.7425963282585144 |\n",
      "| epoch   1 |   283/ 3125 iteration | loss 0.7643147110939026 |\n",
      "| epoch   1 |   284/ 3125 iteration | loss 0.6609885692596436 |\n",
      "| epoch   1 |   285/ 3125 iteration | loss 0.6985946297645569 |\n",
      "| epoch   1 |   286/ 3125 iteration | loss 0.7306728363037109 |\n",
      "| epoch   1 |   287/ 3125 iteration | loss 0.7239181995391846 |\n",
      "| epoch   1 |   288/ 3125 iteration | loss 0.7231140732765198 |\n",
      "| epoch   1 |   289/ 3125 iteration | loss 0.7266099452972412 |\n",
      "| epoch   1 |   290/ 3125 iteration | loss 0.690506100654602 |\n",
      "| epoch   1 |   291/ 3125 iteration | loss 0.7012975215911865 |\n",
      "| epoch   1 |   292/ 3125 iteration | loss 0.7140285968780518 |\n",
      "| epoch   1 |   293/ 3125 iteration | loss 0.6798483729362488 |\n",
      "| epoch   1 |   294/ 3125 iteration | loss 0.701639711856842 |\n",
      "| epoch   1 |   295/ 3125 iteration | loss 0.7029838562011719 |\n",
      "| epoch   1 |   296/ 3125 iteration | loss 0.6721696853637695 |\n",
      "| epoch   1 |   297/ 3125 iteration | loss 0.6722662448883057 |\n",
      "| epoch   1 |   298/ 3125 iteration | loss 0.7001094818115234 |\n",
      "| epoch   1 |   299/ 3125 iteration | loss 0.7188880443572998 |\n",
      "| epoch   1 |   300/ 3125 iteration | loss 0.7072978019714355 |\n",
      "| epoch   1 |   301/ 3125 iteration | loss 0.6733527779579163 |\n",
      "| epoch   1 |   302/ 3125 iteration | loss 0.7029215693473816 |\n",
      "| epoch   1 |   303/ 3125 iteration | loss 0.7640953063964844 |\n",
      "| epoch   1 |   304/ 3125 iteration | loss 0.6876929402351379 |\n",
      "| epoch   1 |   305/ 3125 iteration | loss 0.7165291905403137 |\n",
      "| epoch   1 |   306/ 3125 iteration | loss 0.6715647578239441 |\n",
      "| epoch   1 |   307/ 3125 iteration | loss 0.682592511177063 |\n",
      "| epoch   1 |   308/ 3125 iteration | loss 0.7725761532783508 |\n",
      "| epoch   1 |   309/ 3125 iteration | loss 0.7572859525680542 |\n",
      "| epoch   1 |   310/ 3125 iteration | loss 0.7239091992378235 |\n",
      "| epoch   1 |   311/ 3125 iteration | loss 0.7321189045906067 |\n",
      "| epoch   1 |   312/ 3125 iteration | loss 0.6529091596603394 |\n",
      "| epoch   1 |   313/ 3125 iteration | loss 0.6768868565559387 |\n",
      "| epoch   1 |   314/ 3125 iteration | loss 0.6923699378967285 |\n",
      "| epoch   1 |   315/ 3125 iteration | loss 0.7747595906257629 |\n",
      "| epoch   1 |   316/ 3125 iteration | loss 0.755860447883606 |\n",
      "| epoch   1 |   317/ 3125 iteration | loss 0.7143775224685669 |\n",
      "| epoch   1 |   318/ 3125 iteration | loss 0.7359039783477783 |\n",
      "| epoch   1 |   319/ 3125 iteration | loss 0.682331919670105 |\n",
      "| epoch   1 |   320/ 3125 iteration | loss 0.6897107362747192 |\n",
      "| epoch   1 |   321/ 3125 iteration | loss 0.6977177262306213 |\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[116], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m output \u001b[38;5;241m=\u001b[39m output[:,\u001b[38;5;241m0\u001b[39m,:]\n\u001b[0;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, targets\u001b[38;5;241m.\u001b[39mlong())\n\u001b[1;32m---> 14\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     17\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mD:\\code\\NLP\\Transformers\\New folder\\env\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\code\\NLP\\Transformers\\New folder\\env\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "for epoch in range(10):  # Num of epochs\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        inputs, targets = batch['ids'], batch['targets']\n",
    "        optimizer.zero_grad()\n",
    "        src_mask = model.generate_square_subsequent_mask(inputs.size(0))\n",
    "        output = model(inputs, src_mask)\n",
    "        output = output[:,0,:]\n",
    "        loss = criterion(output, targets.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_value = loss.item()\n",
    "        # log_interval = 200\n",
    "        # if i % log_interval == 0 and i > 0:\n",
    "        # cur_loss = total_loss / log_interval\n",
    "        print('| epoch {:3d} | {:5d}/{:5d} iteration | '\n",
    "                  'loss {} |'.format(epoch+1, i, len(train_dataloader), loss_value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "9f87787c-02a7-43a4-aeed-4818df5acc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"fucking diaster\"\n",
    "\n",
    "# Interpret the predicted class\n",
    "enc_sen = tokenizer.encode(sentence)\n",
    "tensor_enc_sen = torch.LongTensor(enc_sen)\n",
    "tensor_enc_sen_unsqueeze = tensor_enc_sen.unsqueeze(0)\n",
    "mask_tensor_enc_sen = model.generate_square_subsequent_mask(tensor_enc_sen_unsqueeze.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "5f62cb6e-a3a5-4558-a72c-198733406aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(tensor_enc_sen_unsqueeze , mask_tensor_enc_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7ea47a3a-84c5-4fb2-b119-7b32947a66bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0819, 0.3412]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = out[:,0,:]\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "78176ca9-8c0a-4067-91d1-8f035d7666c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4355, 0.5645]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities = softmax(prediction , dim = 1)\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9c36c87b-dc14-4b78-87f8-6e7232b4da8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class = torch.argmax(probabilities, dim=1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "67702902-3f2e-40ac-a2a9-94b34b961c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "6d164fca-4904-4cf4-a7b6-513a222994a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence is classified as positive.\n"
     ]
    }
   ],
   "source": [
    "if predicted_class == 0:\n",
    "    print(\"The sentence is classified as negative.\")\n",
    "else:\n",
    "    print(\"The sentence is classified as positive.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca286553-14c7-46d9-842d-5806908fc0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
